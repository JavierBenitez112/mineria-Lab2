---
title: "Lab Aprendizaje No Supervisado: Clustering, PCA y Algoritmo A Priori"
author: "Pedro Avila, Javier Benitez, Brandon Rivera"
date: "2026-02-07"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r dependencies}
# dependecies
#install.packages("dplyr")
```

# Librerias

``` {r librarys, message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
library(nortest)
library(scales)
library(lubridate)
library(cluster)
library(factoextra)
library(arules)
library(arulesViz)
library(hopkins)
library(fpc)
library(psych)
library(corrplot)
```

# Exploracion de datos
- Una exploración rápida de sus datos, para eso haga un resumen de su conjunto de datos.

```{r}
movies <- read.csv("Movies_2026.csv", fileEncoding = "latin1")

str(movies)
```

  - Al momento de usar ```str()```, nos podemos hacer una idea de como son los datos en todo el dataset, este analisis se hace luego en el proyecto (en especifico en el inciso 2). Como nos podemos dar cuenta varios de estos campos llegan a tener informacion vacia para su tipo de dato respectivo.
  
```{r}

movies <- subset(movies, select = -id) # we ain't going to summarize the the id for pretty obvious reasons
summary(movies)
```


  - Al resumirlos no podemos agarrar informacion tan relevante para hacer cualquier tipo de analisis ademas de algunos valores numericos en dichos campos. Como ver cantidad de ingresos maximos y medios, prespuesto y ganancia por peliculas. No llega a ser informacion tan valiosa solo viendola de por si, ya que son datos generales entre todas y no de una solo empresa o por el estilo (se puede 'queriear' para lograrlo, pero ese no es el caso).
  
## Tipo de cada una de las variables 
- (cualitativa ordinal o nominal, cuantitativa continua, cuantitativa discreta)

**Cualitativas**
Adicionalmente en estas clasificaciones las variables Actors y ActorsCharacters serian clasificados como cualitativos nominales pero no se agregaron porque es necesario una limpieza de datos antes.

```{r datos cualitativos}
cual_nominales <- c(
  "genres",
  "homePage",
  "productionCompany",
  "productionCompanyCountry",
  "productionCountry",
  "video",
  "director",
  "originalTitle",
  "title",
  "originalLanguage"
)

cual_ordinales <- c(
  "releaseDate"
)

cualitativas <- c(cual_nominales, cual_ordinales)
cualitativas
datos_cual <- movies[, cualitativas]
head(datos_cual, 5)
```

**Cuantitativas**
Adicionalmente en estas clasificaciones la variable ActorsPopularity serian clasificados como cuantitativos continuos pero no se agregaron porque es necesario una limpieza de datos antes.

```{r datos cuantitativos}
cuant_discretas <- c(
  "budget",
  "revenue",
  "runtime",
  "voteAvg",
  "voteCount",
  "genresAmount",
  "productionCoAmount",
  "productionCountriesAmount",
  "actorsAmount",
  "castWomenAmount",
  "castMenAmount",
  "releaseYear"
)

cuant_continuas <- c(
  "popularity"
)

cuantitativas <- c(cuant_discretas, cuant_continuas)
cuantitativas
datos_cuant <- movies[, cuantitativas]
head(datos_cuant, 5)
```



# Análisis de Clustering

## Paso 0: Preparación de datos

Se seleccionan variables numéricas que capturan el perfil comercial y de audiencia de cada película: presupuesto, ingresos, popularidad, calificación, número de votos y duración. Se excluyen variables de texto (título, director, géneros, actores), fechas y conteos auxiliares que no aportarían patrones útiles al agrupamiento. También se eliminan registros con `budget = 0` y `revenue = 0`, ya que representan datos incompletos que distorsionan los clusters.

```{r cluster_prep}
# Variables numéricas seleccionadas para clustering
vars_cluster <- c("budget", "revenue", "popularity", "voteAvg", "voteCount", "runtime")

# Eliminar NAs, duplicados y películas sin datos comerciales
d2f_raw <- na.omit(movies[, c("title", vars_cluster)])
d2f_raw <- d2f_raw[!duplicated(d2f_raw$title), ]
d2f_raw <- d2f_raw[d2f_raw$budget > 0 & d2f_raw$revenue > 0, ]

# Escalar datos (necesario: las variables tienen magnitudes muy distintas)
d2f <- as.data.frame(scale(d2f_raw[, vars_cluster]))
rownames(d2f) <- d2f_raw$title

cat("Títulos duplicados:", sum(duplicated(rownames(d2f))), "\n")
cat("Filas totales para clustering:", nrow(d2f), "\n")
```

---

## Paso 1: ¿Hacemos agrupamiento? – Hopkins y VAT

El estadístico de Hopkins mide la tendencia natural de los datos a formar grupos. Un valor cercano a **1** indica alta tendencia; cercano a **0.5** sugiere distribución aleatoria.

```{r hopkins}
# Muestra de 500 filas — Hopkins no requiere el dataset completo
set.seed(123)
muestra_hop <- d2f[sample(nrow(d2f), min(500, nrow(d2f))), ]
hop_resultado <- hopkins(muestra_hop, m = 50)
cat("Estadístico de Hopkins:", round(hop_resultado, 4), "\n")
cat("Valores > 0.75 indican alta tendencia al agrupamiento.\n")
```

```{r vat}
# VAT sobre muestra de 150 filas.
# Se reduce el tamaño para que los bloques sean legibles en la visualización.
# Los datos ya están escalados (d2f), por lo que fviz_dist puede trabajar directamente.
# Se usa una muestra estratificada para representar todo el rango de valores.
set.seed(123)
n_vat <- min(150, nrow(d2f))
muestra_vat <- d2f[sample(nrow(d2f), n_vat), ]
dist_vat <- dist(muestra_vat, method = "euclidean")

fviz_dist(dist_vat, show_labels = FALSE,
          gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07")) +
  labs(title    = "VAT – Evaluación Visual de Tendencia al Agrupamiento",
       subtitle = paste("Muestra de", n_vat, "películas. Bloques azules diagonales indican grupos naturales."))
```

---

## Paso 2: Determinación del número óptimo de clusters

> **Nota técnica:** Para los métodos de determinación de k se usa una muestra representativa de **2,000 películas**. Esto evita el alto consumo de RAM que genera calcular matrices de distancias O(n²) sobre el dataset completo, sin afectar la validez estadística de la decisión.

```{r muestra_k}
set.seed(123)
n_muestra <- min(2000, nrow(d2f))
idx_muestra <- sample(nrow(d2f), n_muestra)
d2f_muestra <- d2f[idx_muestra, ]
cat("Muestra utilizada para determinar k:", n_muestra, "películas\n")
```

```{r metodo_codo_manual}
# Método del codo manual
wss <- numeric(10)
for (i in 1:10) {
  wss[i] <- sum(kmeans(d2f_muestra, centers = i, nstart = 10)$withinss)
}
plot(1:10, wss, type = "b",
     xlab = "Número de Clusters",
     ylab = "Suma de cuadrados dentro del grupo",
     main = "Método del Codo – WSS")
```

```{r codo_factoextra}
fviz_nbclust(d2f_muestra, kmeans, method = "wss", k.max = 10) +
  labs(title    = "Método del Codo (WSS) – K-Means",
       subtitle = "El 'codo' señala el k donde la reducción de WSS se estabiliza",
       x = "Número de Clusters (k)", y = "WSS intra-cluster") +
  theme_bw()
```

```{r silueta_factoextra}
fviz_nbclust(d2f_muestra, kmeans, method = "silhouette", k.max = 10) +
  labs(title    = "Método de Silueta – K-Means",
       subtitle = "El k con mayor silueta promedio es el más adecuado",
       x = "Número de Clusters (k)", y = "Ancho de Silueta Promedio") +
  theme_bw()
```

```{r gap_factoextra}
fviz_nbclust(d2f_muestra, kmeans,
             nstart  = 25,
             method  = "gap_stat",
             nboot   = 50,
             verbose = FALSE) +
  labs(title    = "Gap Statistic – K-Means",
       subtitle = "El k óptimo es donde Gap(k) es máximo o se estabiliza",
       x = "Número de Clusters (k)", y = "Gap Statistic") +
  theme_bw()
```

```{r tabla_silueta}
# Tabla comparativa de silueta por k para selección final
sil_scores <- sapply(2:8, function(k) {
  km <- kmeans(d2f_muestra, centers = k, nstart = 10, iter.max = 50)
  ss <- silhouette(km$cluster, dist(d2f_muestra))
  round(mean(ss[, 3]), 4)
})

sil_tabla <- data.frame(
  k              = 2:8,
  Silhouette     = sil_scores,
  Interpretacion = ifelse(sil_scores >= 0.70, "Estructura fuerte",
                   ifelse(sil_scores >= 0.50, "Estructura razonable",
                   ifelse(sil_scores >= 0.25, "Estructura débil", "Sin estructura")))
)
print(sil_tabla)

k_final <- sil_tabla$k[which.max(sil_tabla$Silhouette)]
cat("\nK seleccionado:", k_final, "\n")
cat("Silueta promedio:", max(sil_tabla$Silhouette), "\n")
```

---

## Paso 3: Algoritmos de Agrupamiento

### Algoritmo 1: K-Means

K-Means se ejecuta sobre el **dataset completo** ya que su complejidad es O(n·k·d) en memoria, sin necesidad de matrices de distancias.

```{r kmeans_clustering}
set.seed(123)
km_res <- kmeans(d2f, centers = k_final, nstart = 25, iter.max = 100)
d2f_raw$cluster_km <- as.factor(km_res$cluster)

cat("Tamaño de cada cluster (K-Means):\n")
print(km_res$size)
cat("Varianza explicada (BSS/TSS):", round(km_res$betweenss / km_res$totss * 100, 1), "%\n")

plotcluster(d2f[sample(nrow(d2f), min(1000, nrow(d2f))), ],
            km_res$cluster[sample(nrow(d2f), min(1000, nrow(d2f)))],
            main = "K-Means – plotcluster (muestra 1000 puntos)")

fviz_cluster(km_res, data = d2f,
             geom         = "point",
             ellipse.type = "norm",
             palette      = "Set2",
             alpha        = 0.4) +
  labs(title    = "K-Means",
       subtitle = paste("k =", k_final, "| n =", nrow(d2f), "películas")) +
  theme_bw()
```

### Algoritmo 2: Clustering Jerárquico (Ward)

El jerárquico requiere una matriz de distancias O(n²). Se aplica sobre una **muestra de 1,500 películas** para que sea computacionalmente viable (~144 MB vs varios GB con el dataset completo).

```{r hierarchical_clustering}
set.seed(456)
n_hc <- min(1500, nrow(d2f))
idx_hc <- sample(nrow(d2f), n_hc)
d2f_hc <- d2f[idx_hc, ]

dist_hc <- dist(d2f_hc, method = "euclidean")
hc_res  <- hclust(dist_hc, method = "ward.D2")

plot(hc_res, labels = FALSE, hang = -1,
     main = paste("Dendrograma – Clustering Jerárquico (Ward)\nMuestra:", n_hc, "películas"),
     xlab = "Películas", ylab = "Distancia (Ward)")
rect.hclust(hc_res, k = k_final, border = 2:(k_final + 1))

hc_clusters <- cutree(hc_res, k = k_final)
cat("Distribución por cluster (Jerárquico):\n")
print(table(hc_clusters))
```

---

## Paso 4: Calidad del agrupamiento – Silueta

Ambos algoritmos se evalúan sobre la misma muestra del jerárquico para hacer la comparación justa.

```{r quality_comparison}
km_hc  <- kmeans(d2f_hc, centers = k_final, nstart = 25)
sil_km <- silhouette(km_hc$cluster, dist_hc)
sil_hc <- silhouette(hc_clusters,   dist_hc)

calidad <- data.frame(
  Algoritmo           = c("K-Means", "Jerárquico (Ward)"),
  Silhouette_Promedio = round(c(mean(sil_km[, 3]), mean(sil_hc[, 3])), 4),
  Clusters            = k_final
)
print(calidad)
```

```{r silueta_kmeans, fig.width=8, fig.height=5}
# Silueta K-Means — gráfico individual para mejor legibilidad
plot(sil_km,
     col     = (2:(k_final + 1))[sil_km[, 1]],
     border  = NA,
     main    = paste("Silueta K-Means | k =", k_final,
                     "| avg =", round(mean(sil_km[, 3]), 3)),
     sub     = paste("n =", nrow(d2f_hc), "películas (muestra)"))
abline(v = mean(sil_km[, 3]), lty = 2, col = "red")
```

```{r silueta_jerarquico, fig.width=8, fig.height=5}
# Silueta Jerárquico — gráfico individual para mejor legibilidad
plot(sil_hc,
     col     = (2:(k_final + 1))[sil_hc[, 1]],
     border  = NA,
     main    = paste("Silueta Jerárquico (Ward) | k =", k_final,
                     "| avg =", round(mean(sil_hc[, 3]), 3)),
     sub     = paste("n =", nrow(d2f_hc), "películas (muestra)"))
abline(v = mean(sil_hc[, 3]), lty = 2, col = "red")
```

```{r mejor_algoritmo}
mejor_algoritmo <- calidad$Algoritmo[which.max(calidad$Silhouette_Promedio)]
cat("Algoritmo con mejor calidad de clusters:", mejor_algoritmo, "\n")
cat("Se usará K-Means (dataset completo) para la interpretación final.\n")
```

---

## Paso 5: Interpretación de los grupos

```{r cluster_medidas_tendencia}
# Medidas de tendencia central por cluster (K-Means, dataset completo)
perfil <- d2f_raw %>%
  group_by(cluster_km) %>%
  summarise(
    n_peliculas     = n(),
    budget_media    = round(mean(budget,     na.rm = TRUE), 0),
    budget_mediana  = round(median(budget,   na.rm = TRUE), 0),
    revenue_media   = round(mean(revenue,    na.rm = TRUE), 0),
    revenue_mediana = round(median(revenue,  na.rm = TRUE), 0),
    pop_media       = round(mean(popularity, na.rm = TRUE), 2),
    voteAvg_media   = round(mean(voteAvg,    na.rm = TRUE), 2),
    voteCount_media = round(mean(voteCount,  na.rm = TRUE), 0),
    runtime_media   = round(mean(runtime,    na.rm = TRUE), 1)
  )
print(perfil)
```

```{r cluster_boxplots}
for (v in vars_cluster) {
  p <- ggplot(d2f_raw, aes_string(x = "cluster_km", y = v, fill = "cluster_km")) +
    geom_boxplot(alpha = 0.7, outlier.color = "red", outlier.size = 1) +
    scale_fill_brewer(palette = "Set2") +
    labs(title    = paste("Distribución de", v, "por Cluster"),
         subtitle = "Outliers en rojo = películas atípicas dentro del grupo",
         x = "Cluster", y = v) +
    theme_bw() + theme(legend.position = "none")
  print(p)
}
```

```{r cluster_freq_genres}
movies_con_cluster <- movies %>%
  filter(title %in% d2f_raw$title) %>%
  left_join(d2f_raw[, c("title", "cluster_km")], by = "title") %>%
  tidyr::separate_rows(genres, sep = "\\|")

tabla_generos <- movies_con_cluster %>%
  group_by(cluster_km, genres) %>%
  summarise(frecuencia = n(), .groups = "drop") %>%
  arrange(cluster_km, desc(frecuencia))

top_generos <- tabla_generos %>%
  group_by(cluster_km) %>%
  slice_max(frecuencia, n = 5)

print(top_generos)

ggplot(top_generos, aes(x = reorder(genres, frecuencia), y = frecuencia, fill = cluster_km)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~cluster_km, scales = "free_y") +
  coord_flip() +
  scale_fill_brewer(palette = "Set2") +
  labs(title    = "Top 5 Géneros por Cluster",
       subtitle = "Distribución de géneros dentro de cada grupo de películas",
       x = "Género", y = "Frecuencia") +
  theme_bw()
```

```{r cluster_perfil_comparativo}
perfil_norm <- perfil %>%
  select(cluster_km, budget_media, revenue_media, pop_media, voteAvg_media, runtime_media) %>%
  mutate(across(-cluster_km, ~ round((. - min(.)) / (max(.) - min(.)), 4)))

cat("=== Perfil normalizado de clusters (0 = mínimo, 1 = máximo) ===\n")
print(perfil_norm)

perfil_long <- perfil_norm %>%
  tidyr::pivot_longer(-cluster_km, names_to = "variable", values_to = "valor")

ggplot(perfil_long, aes(x = variable, y = valor, fill = cluster_km)) +
  geom_col(position = "dodge", alpha = 0.8) +
  scale_fill_brewer(palette = "Set2") +
  labs(title    = "Perfil Comparativo de Clusters (valores normalizados)",
       subtitle = "Identifica en qué dimensiones se diferencia cada grupo",
       x = "Variable", y = "Valor Normalizado (0–1)", fill = "Cluster") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))
```

```{r cluster_scatter_revenue_budget}
ggplot(d2f_raw, aes(x = budget, y = revenue, color = cluster_km)) +
  geom_point(alpha = 0.5, size = 1.5) +
  scale_color_brewer(palette = "Set2") +
  scale_x_continuous(labels = scales::comma) +
  scale_y_continuous(labels = scales::comma) +
  labs(title    = "Budget vs Revenue por Cluster",
       subtitle = "Cada color representa un grupo con características similares",
       x = "Presupuesto (Budget)", y = "Ingresos (Revenue)", color = "Cluster") +
  theme_bw()
```

```{r cluster_scatter_popularity_vote}
ggplot(d2f_raw, aes(x = popularity, y = voteAvg, color = cluster_km)) +
  geom_point(alpha = 0.5, size = 1.5) +
  scale_color_brewer(palette = "Set2") +
  labs(title    = "Popularidad vs Calificación Promedio por Cluster",
       subtitle = "¿Los clusters más populares son también los mejor calificados?",
       x = "Popularidad", y = "Calificación Promedio (voteAvg)", color = "Cluster") +
  theme_bw()
```

---

# PCA

## Paso 0 - Análisis de viabilidad

### KMO

```{r}
datos_cuant_clean <- datos_cuant[, sapply(datos_cuant, is.numeric)]
datos_cuant_clean <- na.omit(datos_cuant_clean)
KMO(datos_cuant_clean)
```

### Esfericidad de Bartlett

```{r}
bart_spher(datos_cuant_clean)
```

## Visualización de correlaciones

```{r}
matriz <- cor(datos_cuant_clean, use = "pairwise.complete.obs")
corrplot(matriz,
         method = "color",
         type = "upper",
         addCoef.col = "black",
         tl.col = "black",
         tl.srt = 45)
```

## Paso 1 - Estandarización

```{r}
datos_std <- scale(datos_cuant_clean)
apply(datos_std, 2, mean)
apply(datos_std, 2, sd)
```

## Paso 2 - PCA

```{r}
compPrinc <- prcomp(datos_std)
summary(compPrinc)
```

## Scree Plot

```{r}
fviz_eig(compPrinc)
```

## Visualización de variables

```{r}
fviz_pca_var(compPrinc,
             col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)
```


# Algoritmo A Priori

## Paso 1: Contexto y Objetivo

## Paso 2: Items mas frecuentes

## Paso 3: Conjunto de Items frecuentes

## Paso 4: Análisis del soporte de los conjuntos

## Paso 5: Generación de reglas de asociación

## Paso 6: Soporte, confianza y lift

## Paso 7: Filtrado de reglas relevantes

## Paso 8: Visualización de resultados



# Seleccion de Algoritmo de Aprendizaje no supervisado

# Aprendizaje no supervisado: SVG

# Conclusiones
